---
title: "ISTfinalPaper_text_analysis"
---

```{r environment}
library(readr)
library(dplyr)
library(data.table)
library(statnet)
library(lubridate)
library(ggplot2)
```

```{r load and subset data}
tweets <- read.delim("~/Box Sync/Python codes/twecoll/", header=FALSE, sep=",")

#names(tweets)

data <- select(tweets, user, text, geo, place, timestamp_ms)

towns<-c("Ithaca, NY", "State College, PA", "Bloomington, IN", "Lawrence,
         KS", "Blacksburg, VA", "College Station, TX", "Columbia, MO",
         "Champaign, IL", "Ann Arbor, MI", "Gainesville, FL")

data_college_town <-data[grep(paste(towns,collapse="|"),
                                                   data$place$full_name),]

data_college_town$location <- data_college_town$place$full_name
data_college_town$ID <- data_college_town$user$id
data_college_town$user <- data_college_town$user$screen_name
# data_college_town$coordinates <- data_college_town$geo$coordinates

tweets_college_town <- select(data_college_town, location, ID, user, text, timestamp_ms)
```

Remove bots from the data
```{r}
#botcleaned users
load("~/Downloads/user_locations_botcleaned.Rdata")

botCleaned <- user_locations_cleaned
botCleaned$location_num<-rep(NA,1578)
for(i in 1:1578){
  botCleaned$location_num[i]<-length(user_locations_cleaned$locations[[i]])
}
botCleaned<-botCleaned[botCleaned$location_num<=25,]

tweets_college_town <- left_join(tweets_college_town, botCleaned, by="user")
tweets_college_town <- subset(tweets_college_town, !is.na(probab))
tweets_college_town <- subset(tweets_college_town, probab<0.59)

tweets_college_town[6:7] <- NULL
tweets_college_town <- tbl_df(tweets_college_town)

#eliminating noticeable companies
company <- c("UMtransit", "countryrecruits", "dominos", "Coyote_Careers", "_ChampaignIL", "hucksbeerbuzz", "SwampHead", "tmj_il_vets", "_GainesvilleFL", "PlastipakJobs")
for (i in company) {
  tweets_college_town <- subset(tweets_college_town, user!=i)
}

#View(tweets_college_town)
```

I will start with removing stop words.
```{r clean data}
library(tidytext)
library(stringr)

replace_reg <- "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https"
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"

#I want to eliminate some unmeaningful words like tweet, est, mi, fl, etc. (from the most frequent appearing words)

#glimpse(stop_words)
#stop_words$lexicon

myStopWords <- matrix(c("tweet", "est", "va", "fl", "tx", "fl", "il", "pa", "mi", "in", 1:10),ncol=2,byrow=FALSE)
colnames(myStopWords) <- c("word", "lexicon")
myStopWords <- tbl_df(myStopWords)
mstop_words <- tbl_df(stop_words)
myStopWords <- bind_rows(mstop_words, myStopWords)

tidy_tweets <- tweets_college_town %>% 
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = unnest_reg) %>%
  filter(!word %in% myStopWords$word,
         str_detect(word, "[a-z]"))

names(tidy_tweets)
```

Let's look at simple frequency
```{r simple frequency}
library(tidyverse)
library(xtable)
frequency <- tidy_tweets %>% 
  group_by(location) %>% 
  count(word, sort = TRUE) %>% 
  left_join(tidy_tweets %>% 
              group_by(location) %>% 
              summarise(total = n())) %>%
  mutate(freq = n/total) %>%
  tbl_df()

glimpse(frequency)

#delete the location names and hiring/job
delete <- c("#annarbor", "ann", "arbor", "michigan", "gainesville", "florida", "college", "station", "texas", "champaign", "urbana", "illinois", "state", "pennsylvania", "blacksburg", "virginia", "#champaign", "hiring", "job", "#hiring", "#job", "tech", "#manufacturing", "#veterans", "#jobs", "#supplychain")

for (i in delete) {
  frequency <- subset(frequency, word!=i)
}
frequency <- frequency[order(-frequency$freq),] 

topFrequency <- slice(frequency, 1:15)
xtable(topFrequency)

#slice dataset by location
AnnArbor <- subset(frequency, location=="Ann Arbor, MI")
Gainesville <- subset(frequency, location=="Gainesville, FL")
CollegeStation <- subset(frequency, location=="College Station, TX")
Champaign <- subset(frequency, location=="Champaign, IL")
StateCollege <- subset(frequency, location=="State College, PA")
Blacksburg <- subset(frequency, location=="Blacksburg, VA")

locations <- list(AnnArbor, Blacksburg, Champaign, CollegeStation, Gainesville, StateCollege)

topFrequency1 <- NULL
for (i in locations){
topFrequency2 <- slice(i, 1:10)
topFrequency2 <- tbl_df(topFrequency2$word)
topFrequency1 <- bind_cols(topFrequency1, topFrequency2)
}

#Champaign has la as 10th top word. replace it with the 11th
#topFrequency2 <- slice(Champaign, 1:11)
#topFrequency2 #people is the 11th word
#State College has posted as 10th top word. replace it with the 11th
#topFrequency2 <- slice(StateCollege, 1:11)
#topFrequency2 #bryce is the 11th word

xtable(topFrequency1)
```

```{r graphs}
library(scales)

colnames(frequency2) <- c("word", "AnnArbor", "Blacksburg", "Champaign", "CollegeStation", "Gainesville", "StateCollege")

#Do we want to compare college town to college town? I could not find a nice way to plot these yet
ggplot(frequency2, aes(AnnArbor, Blacksburg)) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  geom_abline(color = "red")

ggplot(frequency2, aes(AnnArbor, StateCollege)) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  geom_abline(color = "red")

ggplot(frequency2, aes(Blacksburg, StateCollege)) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  geom_abline(color = "red")
```

```{r wordcloud}
library(wordcloud2)

head(CollegeStation)

AnnArbor2 <- dplyr::select(AnnArbor, word, freq)
CollegeStation2 <- dplyr::select(CollegeStation, word, freq)
StateCollege2 <- dplyr::select(StateCollege, word, freq)
Blacksburg2 <- dplyr::select(Blacksburg, word, freq)
Gainesville2 <- dplyr::select(Gainesville, word, freq)
Champaign2 <- dplyr::select(Champaign, word, freq)

wordCloud <- function(filename){
  wordcloud2(data=filename, size = 1, minSize = 0, gridSize =  0,
    fontFamily = 'Segoe UI', fontWeight = 'bold',
    color = 'random-dark', backgroundColor = "white",
    minRotation = -pi/4, maxRotation = pi/4, shuffle = TRUE,
    rotateRatio = 0.4, shape = 'circle', ellipticity = 0.65,
    widgetsize = NULL, figPath = NULL, hoverFunction = NULL)
}

#save images to pdf
library(webshot)
webshot::install_phantomjs()
# Make the graph
g1<-wordCloud(AnnArbor2)
g2<-wordCloud(Gainesville2)
g3<-wordCloud(CollegeStation2)
g4<-wordCloud(Champaign2)
g5<-wordCloud(StateCollege2)
g6<-wordCloud(Blacksburg2)

# save it in html
library("htmlwidgets")

setwd("~/Box Sync/2018 Spring/SoDA 501/FinalProject")

saveWidget(g1,"g1.html",selfcontained = F)
saveWidget(g2,"g2.html",selfcontained = F)
saveWidget(g3,"g3.html",selfcontained = F)
saveWidget(g4,"g4.html",selfcontained = F)
saveWidget(g5,"g5.html",selfcontained = F)
saveWidget(g6,"g6.html",selfcontained = F)

webshot("g1.html","fig_1.pdf", delay =20, vwidth = 480, vheight=480)
webshot("g2.html","fig_2.pdf", delay =20, vwidth = 480, vheight=480)
webshot("g3.html","fig_3.pdf", delay =20, vwidth = 480, vheight=480)
webshot("g4.html","fig_4.pdf", delay =20, vwidth = 480, vheight=480)
webshot("g5.html","fig_5.pdf", delay =20, vwidth = 480, vheight=480)
webshot("g6.html","fig_6.pdf", delay =20, vwidth = 480, vheight=480)
# and in png
#webshot("tmp.html","fig_1.pdf", delay =5, vwidth = 480, vheight=480)
```