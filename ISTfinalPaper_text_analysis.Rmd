---
title: "ISTfinalPaper_text_analysis"
---

```{r environment}
library(readr)
library(dplyr)
library(data.table)
library(statnet)
library(lubridate)
library(ggplot2)
```

```{r load and subset data}
discoverPHL <- read_delim("~/Box Sync/Python codes/GetOldTweets-python-master/discoverPHL.csv", ";", escape_double = FALSE, trim_ws = TRUE)

KKoreanTravel <- read_csv("~/Box Sync/Python codes/GetOldTweets-python-master/KoreanTravel.csv")

ExploreCHS <- read_delim("~/Box Sync/Python codes/GetOldTweets-python-master/ExploreCHS.csv", ";", escape_double = FALSE, trim_ws = TRUE)

visitPA <- read_delim("~/Box Sync/Python codes/GetOldTweets-python-master/visitPA.csv", ";", escape_double = FALSE, trim_ws = TRUE)
#names(KoreanTravel)

names<-c("discoverPHL", "KoreanTravel", "ExploreCHS", "visitPA")
```

I will start with removing stop words.
```{r clean data}
library(tidytext)
library(stringr)

replace_reg <- c("https://t.co/", "http://", "&amp;", "&lt;", "&gt;", "RT", "https")

Atxt <- discoverPHL$text
Btxt <- KoreanTravel$text
Ctxt <- ExploreCHS$text
Dtxt <- visitPA$text

rmNonAlphabet <- function(str) {
  words <- unlist(strsplit(str, " "))
  in.alphabet <- grep(words, pattern = "[a-z|0-9]", ignore.case = T)
  nice.str <- paste(words[in.alphabet], collapse = " ")
  nice.str
}
```

```{r wordcloud function}
library(wordcloud2)

wordCloud <- function(filename){
  wordcloud2(data=filename, size = 1, minSize = 0, gridSize =  0,
    fontFamily = 'Segoe UI', fontWeight = 'bold',
    color = 'random-dark', backgroundColor = "white",
    minRotation = -pi/4, maxRotation = pi/4, shuffle = TRUE,
    rotateRatio = 0.4, shape = 'circle', ellipticity = 0.65,
    widgetsize = NULL, figPath = NULL, hoverFunction = NULL)
}

```

```{r discoverPHL}
library(quanteda)
library(xtable)
library(tidyverse)
library("htmlwidgets")
library(webshot)

Atxt <- Atxt %>%
  rmNonAlphabet()

delete <- c("http", "https", "discoverphl", "philadelphia", "status", "us", "day") #remove unrelevant words

PHL <- tokens(Atxt) %>%
    tokens_remove("\\p{P}", valuetype = "regex", padding = TRUE) %>%
    tokens_remove(stopwords("english"), padding  = TRUE) %>%
    tokens_remove(delete, padding  = TRUE) %>%
    tokens_remove(replace_reg, padding  = TRUE) %>%
    tokens_ngrams(n = 1) %>%
    dfm()

textstat_lexdiv(PHL)
textstat_frequency(PHL)
xtable(tbl_df(topfeatures(PHL)))
head(PHL)
dPHL <- tbl_df(textstat_frequency(PHL)) #need frequency info
head(dPHL)

#let's create world cloud
dPHL <- dplyr::select(dPHL, feature, frequency)
#save images to pdf
# Make the graph
p1 <- wordCloud(dPHL)
# save it in html
saveWidget(p1,"p1.html",selfcontained = F)
webshot("p1.html","PHL.png", delay =30, vwidth = 480, vheight=480)
```

```{r KoreanTravel}
Btxt <- Btxt %>%
  rmNonAlphabet()

delete <- c("http", "https", "southkorea", "korea", "koreantravel", "infohttp", "status", "=", "south") #remove unrelevant words

KR <- tokens(Btxt, remove_puct = TRUE, remove_numbers = TRUE) %>%
    tokens_remove("\\p{P}", valuetype = "regex", padding = TRUE) %>%
    tokens_remove(stopwords("english"), padding  = TRUE) %>%
    tokens_remove(delete, padding  = TRUE) %>%
    tokens_remove(replace_reg, padding  = TRUE) %>%
    tokens_ngrams(n = 1) %>%
    dfm()

textstat_lexdiv(KR)
textstat_frequency(KR)
xtable(tbl_df(topfeatures(KR)))
head(KR)
dKR <- tbl_df(textstat_frequency(KR)) #need frequency info
dKR <- slice(dKR, 2:10455)
head(dKR)

#let's create world cloud
dKR <- dplyr::select(dKR, feature, frequency)
#save images to pdf
# Make the graph
p2 <- wordCloud(dKR)
# save it in html
#saveWidget(p2,"p2.html",selfcontained = F)
#webshot("p2.html","KR.png", delay =30, vwidth = 480, vheight=480)
```

```{r ExploreCHS}
Ctxt <- Ctxt %>%
  rmNonAlphabet()

delete <- c("http", "https", "charleston", "explorechs", "us", "p", "status", "check") #remove unrelevant words

CHS <- tokens(Ctxt, remove_puct = TRUE, remove_numbers = TRUE) %>%
    tokens_remove("\\p{P}", valuetype = "regex", padding = TRUE) %>%
    tokens_remove(stopwords("english"), padding  = TRUE) %>%
    tokens_remove(delete, padding  = TRUE) %>%
    tokens_remove(replace_reg, padding  = TRUE) %>%
    tokens_ngrams(n = 1) %>%
    dfm()

textstat_lexdiv(CHS)
textstat_frequency(CHS)
xtable(tbl_df(topfeatures(CHS)))
head(CHS)
dCHS <- tbl_df(textstat_frequency(CHS)) #need frequency info
head(dCHS)

#let's create world cloud
dCHS <- dplyr::select(dCHS, feature, frequency)
#save images to pdf
# Make the graph
p3 <- wordCloud(dCHS)
# save it in html
#saveWidget(p3,"p3.html",selfcontained = F)
#webshot("p3.html","CHS.png", delay =30, vwidth = 480, vheight=480)
```

```{r visitPA}
Dtxt <- Dtxt %>%
  rmNonAlphabet()

delete <- c("http", "https", "pennsylvania", "visitpa", "us", "p", "status", "rt", "state", "day", "=", "pa", "c", "check") #remove unrelevant words

PA <- tokens(Dtxt, remove_puct = TRUE, remove_numbers = TRUE) %>%
    tokens_remove("\\p{P}", valuetype = "regex", padding = TRUE) %>%
    tokens_remove(stopwords("english"), padding  = TRUE) %>%
    tokens_remove(delete, padding  = TRUE) %>%
    tokens_remove(replace_reg, padding  = TRUE) %>%
    tokens_ngrams(n = 1) %>%
    dfm()

textstat_lexdiv(PA)
textstat_frequency(PA)
xtable(tbl_df(topfeatures(PA)))
head(PA)
dPA <- tbl_df(textstat_frequency(PA)) #need frequency info
head(dPA)

#let's create world cloud
dPA <- dplyr::select(dPA, feature, frequency)
#save images to pdf
# Make the graph
p4 <- wordCloud(dPA)
# save it in html
#saveWidget(p4,"p4.html",selfcontained = F)
#webshot("p4.html","PA.png", delay =30, vwidth = 480, vheight=480)
```

```{r}
#create one table for all college towns
table <- tbl_df(matrix(c(1:10), ncol=1))
list <- list(KR, PA, PHL, CHS)
for (i in list){
  i <- tbl_df(topfeatures(i))
  i$word <- rownames(i)
  i <- i[c(2,1)]
  table <- cbind(table, i)
}

xtable(table)
```


```{r}
T = matrix(nrow=4, ncol=6)
dimnames(T) = list(
  c("dKR", "dPA", "dPHL", "dCHS"),         # row names 
  c("nature", "shopping", "culture", "heritage", "adventure", "sports")) # column names 

data <- c("dKR", "dPA", "dPHL", "dCHS")
#Calculate overlapping number of users
attributes <- "nature shopping culture heritage adventure sports"
attributes <- strsplit(attributes, " ")[[1]]

for (a in data){
  for (b in attributes){
    i <- get(a)
    T[a,b] <- sum(i$frequency[str_count(i$feature, b)>0])/sum(dKR$frequency)
  }
}

T <- tbl_df(T)
data <- 100*T

data$destination = c("dKR", "dPA", "dPHL", "dCHS")

data <- data %>%
  gather(1:4, key="category", value="number")

#ggplot(data, aes(category, number, color=destination)) +
 # geom_point()

ggplot(data, aes(destination, number, color=destination)) +
  geom_point() + 
  facet_wrap( ~ category) + 
  scale_color_manual(values = brewer.pal(n = 10, name = "RdBu")) +  
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  xlab("Destination") +
  ylab("Overlap")

```

